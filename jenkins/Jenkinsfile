pipeline {
    agent {
        label 'terraform-aws'  // Usar agente específico con herramientas instaladas
    }
    
    environment {
        JOB_NAME = "terraform-aws"
        AWS_REGION = "us-east-1"
        AWS_CREDENTIALS_ID = 'aws-cli-token'
        ANSIBLE_DIR = "../ansible"
        TF_STATE_BUCKET = "tf-state-bucket-${params.ENVIRONMENT}"
        JENKINS_TRIGGER_TOKEN = "JENKINS_TRIGGER_TOKEN"
        SLACK_CHANNEL = '#infra-alerts'
        SLACK_TOKEN_CREDENTIAL_ID = 'slack-token'
        SLACK_TEAM_DOMAIN = 'aws-terraform-ansible'
    }

    parameters {
        string(name: 'GIT_SHA', defaultValue: '', description: 'Commit SHA from GitHub')
        string(name: 'GIT_REF', defaultValue: 'main', description: 'Git reference from GitHub')
        choice(name: 'ENVIRONMENT', choices: ['dev', 'prod'], description: 'Target environment')
    }

    triggers {
        GenericTrigger(
            genericVariables: [
                [
                    key: 'GIT_SHA', 
                    value: '$.sha',
                    expressionType: 'JSONPath',
                    defaultValue: ''
                ],
                [
                    key: 'GIT_REF', 
                    value: '$.ref',
                    expressionType: 'JSONPath',
                    defaultValue: 'main'
                ],
                [
                    key: 'ENVIRONMENT',
                    value: '$.environment',
                    expressionType: 'JSONPath',
                    regexpFilter: 'dev|prod',
                    defaultValue: 'dev'
                ]
            ],
            token: 'terraform-ci',
            printContributedVariables: true,
            silentResponse: false,
            printPostContent: true,
            causeString: 'Triggered by GitHub Actions',
        )
    }

    stages {
        stage('Download Terraform Plan') {
            when {
                expression { params.GIT_SHA?.trim() }
            }
            steps {
                withCredentials([string(credentialsId: 'JENKINS_TRIGGER_TOKEN', variable: 'JENKINS_TRIGGER_TOKEN')]) {
                    script {
                        def artifactsResponse = httpRequest(
                            url: "https://api.github.com/repos/alexhl005/aws-terraform-ansible/actions/artifacts",
                            customHeaders: [
                                [name: 'Authorization', value: "token ${JENKINS_TRIGGER_TOKEN}"],
                                [name: 'Accept', value: 'application/vnd.github.v3+json']
                            ]
                        )
                        def artifacts = readJSON text: artifactsResponse.content
                        def artifact = artifacts.artifacts.find {
                            it.workflow_run?.head_sha == params.GIT_SHA &&
                            it.name.endsWith("-${params.ENVIRONMENT}")
                        }

                        if (!artifact) {
                            error "Artifact no encontrado para SHA ${params.GIT_SHA} y entorno ${params.ENVIRONMENT}"
                        }

                        def artifactUrl = artifact.archive_download_url.replace('/zip', '')
                        echo "Artifact URL: ${artifactUrl}"

                        dir("terraform/environments/${params.ENVIRONMENT}") {
                            sh """
                                curl -sL -H 'Authorization: token ${JENKINS_TRIGGER_TOKEN}' \\
                                    -o tfplan.zip \\
                                    ${artifact.archive_download_url}

                                unzip -o tfplan.zip
                                rm tfplan.zip

                                ls -la
                            """
                        }
                    }
                }
            }
        }

        stage('Prepare Workspace') {
            steps {
                // Asegurarnos de que las carpetas existen
                sh '''
                mkdir -p terraform/environments/${ENVIRONMENT}
                mkdir -p tfstate
                '''
                }
            }

        stage('Terraform Apply') {
            steps {
                script {
                    def userInput = input(
                        id: 'confirm', 
                        message: "¿Aplicar los cambios de Terraform en ${params.ENVIRONMENT}?", 
                        parameters: [
                            text(name: 'reason', description: 'Razón del despliegue', defaultValue: '')
                        ])
                    
                    withCredentials([[
                        $class: 'AmazonWebServicesCredentialsBinding',
                        credentialsId: env.AWS_CREDENTIALS_ID,
                        accessKeyVariable: 'AWS_ACCESS_KEY_ID',
                        secretKeyVariable: 'AWS_SECRET_ACCESS_KEY'
                    ]]) {
                        dir("terraform/environments/${params.ENVIRONMENT}") {
                            sh '''
                            terraform init \
                                -backend-config="bucket=${TF_STATE_BUCKET}" \
                                -backend-config="key=${ENVIRONMENT}/terraform.tfstate" \
                                -backend-config="region=${AWS_REGION}" \
                                -reconfigure
                            '''
                            sh 'terraform apply -input=false -auto-approve tfplan'
                        }
                    }
                }
            }
        }

        stage('Generate Terraform Outputs') {
            steps {
                dir("terraform/environments/${params.ENVIRONMENT}") {
                    sh """
                    terraform output -json > ../../tf_outputs/tf_outputs_${params.ENVIRONMENT}.json
                    """
                }
            }
        }

        stage('Configure Ansible Inventory') {
            steps {
                script {
                    def tfOutputs = readJSON file: "tf_outputs/tf_outputs_${params.ENVIRONMENT}.json"
                    def webIps = tfOutputs.ec2_instances.value.join('\n')
                    def bastionIp = tfOutputs.bastion_public_ip.value

                    writeFile file: "${env.ANSIBLE_DIR}/inventories/${params.ENVIRONMENT}/hosts", 
                        text: """
        [webservers]
        ${webIps}

        [webservers:vars]
        ansible_user=ubuntu
        ansible_ssh_private_key_file=../modules/ec2/.ssh/${params.ENVIRONMENT}-wp-key.pem
        ansible_ssh_common_args='-o ProxyJump=ubuntu@${bastionIp}'
        db_host=${tfOutputs.rds_endpoint.value}
        elb_dns=${tfOutputs.elb_dns_name.value}
        """
                }
            }
        }

        stage('Update Configuration Files') {
            steps {
                script {
                    // 1. Leer los outputs de Terraform
                    def tfOutputs = readJSON file: "tf_outputs/tf_outputs_${params.ENVIRONMENT}.json"
                    def webIps    = tfOutputs.ec2_instances.value   // Lista de IPs privadas de los webservers
                    def bastionIp = tfOutputs.bastion_public_ip.value

                    // 2. Actualizar el archivo de variables de Ansible con el endpoint del cluster
                    def phpConfigFile = "${env.ANSIBLE_DIR}/group_vars/webservers.yml"
                    def phpContent    = readFile(file: phpConfigFile)
                    phpContent         = phpContent.replace("(cluster_endpoint)", tfOutputs.cluster_endpoint.value)
                    writeFile(file: phpConfigFile, text: phpContent)

                    // 3. Actualizar el script bash con el bucket ARN
                    def bashScriptFile = "scripts/bash/backup/s3_sync.sh"
                    def bashContent    = readFile(file: bashScriptFile)
                    bashContent         = bashContent.replace("(bucket_arn)", tfOutputs.bucket_arn.value)
                    writeFile(file: bashScriptFile, text: bashContent)

                    // 4. Confirmar los cambios en consola
                    sh """
                        echo "Archivos actualizados:"
                        echo " - ${phpConfigFile}: DB_HOST ahora es ${tfOutputs.cluster_endpoint.value}"
                        echo " - ${bashScriptFile}: S3_BUCKET ahora es ${tfOutputs.bucket_arn.value}"
                    """

                    // 5. Definir la ruta a la clave privada generada por Terraform
                    def privateKey = "modules/ec2/.ssh/${params.ENVIRONMENT}-wp-key.pem"

                    // 6. Copiar y configurar en cada webserver a través del bastión
                    sh '''
                        #!/bin/bash
                        set -e

                        # Lista de archivos que vamos a copiar y sus rutas destino en la instancia
                        declare -A FILE_MAP=(
                        ["scripts/bash/backup/s3_sync.sh"]="/home/ubuntu/scripts/bash/backup/s3_sync.sh"
                        ["scripts/bash/monitoring/check_services.sh"]="/home/ubuntu/scripts/monitoring/check_services.sh"
                        ["scripts/bash/monitoring/log_analyzer.sh"]="/home/ubuntu/scripts/monitoring/log_analyzer.sh"
                        ["scripts/bash/utilities/cleanup.sh"]="/home/ubuntu/scripts/bash/utilities/cleanup.sh"
                        ["scripts/bash/utilities/security_audit.sh"]="/home/ubuntu/scripts/bash/utilities/security_audit.sh"
                        ["scripts/bash/utilities/weekly_maintenance.sh"]="/home/ubuntu/scripts/bash/utilities/weekly_maintenance.sh"
                        ["scripts/python/cloudwatch/cloudwatch_alerts.py"]="/opt/aws-monitoring/cloudwatch_alerts.py"
                        ["scripts/python/slack_reporter.py"]="/home/ubuntu/scripts/python/slack_reporter.py"
                        ["scripts/python/cloudwatch/cloudwatch-metrics.service"]="/tmp/cloudwatch-metrics.service"
                        )

                        for ip in ${webIps.join(' ')}; do
                        echo "---- Copiando archivos a webserver \$ip vía bastión \$bastionIp ----"

                        for src in "${!FILE_MAP[@]}"; do
                            dst=\${FILE_MAP[\$src]}
                            echo " -> Copiando \$src a \$ip:\$dst"
                            scp -i ${privateKey} -o StrictHostKeyChecking=no \\
                                -o ProxyJump=ubuntu@${bastionIp} \\
                                \${WORKSPACE}/\$src ubuntu@\$ip:\$dst
                        done

                        echo "---- Ajustando permisos e instalando dependencias en \$ip ----"
                        ssh -i ${privateKey} -o StrictHostKeyChecking=no \\
                            -o ProxyJump=ubuntu@${bastionIp} \\
                            ubuntu@\$ip << 'EOF'
                            set -e

                            # 1. Dar permisos de ejecución a los scripts
                            sudo chmod +x /home/ubuntu/scripts/bash/backup/s3_sync.sh
                            sudo chmod +x /home/ubuntu/scripts/monitoring/check_services.sh
                            sudo chmod +x /home/ubuntu/scripts/monitoring/log_analyzer.sh
                            sudo chmod +x /home/ubuntu/scripts/bash/utilities/cleanup.sh
                            sudo chmod +x /home/ubuntu/scripts/bash/utilities/security_audit.sh
                            sudo chmod +x /home/ubuntu/scripts/bash/utilities/weekly_maintenance.sh
                            sudo chmod +x /opt/aws-monitoring/cloudwatch_alerts.py
                            sudo chmod +x /home/ubuntu/scripts/python/slack_reporter.py

                            # 2. Mover el servicio de CloudWatch a systemd y habilitarlo
                            sudo mv /tmp/cloudwatch-metrics.service /etc/systemd/system/
                            sudo systemctl daemon-reload
                            sudo systemctl enable cloudwatch-metrics
                            sudo systemctl start cloudwatch-metrics

                            # 3. Instalar pip3 y dependencias Python
                            sudo apt-get update -y
                            sudo apt-get install -y python3-pip
                            sudo pip3 install boto3 psutil requests

                            # 4. Crear cron jobs
                            ( crontab -l 2>/dev/null; echo "59 23 * * 0 /home/ubuntu/scripts/bash/backup/s3_sync.sh" ) | crontab -
                            ( crontab -l 2>/dev/null; echo "*/5 * * * * /home/ubuntu/scripts/monitoring/check_services.sh" ) | crontab -
                            ( crontab -l 2>/dev/null; echo "0 1 * * * /home/ubuntu/scripts/monitoring/log_analyzer.sh" ) | crontab -
                            ( crontab -l 2>/dev/null; echo "0 0 * * * /home/ubuntu/scripts/bash/utilities/cleanup.sh" ) | crontab -
                            ( crontab -l 2>/dev/null; echo "0 12 * * 1 /home/ubuntu/scripts/bash/utilities/security_audit.sh" ) | crontab -
                            ( crontab -l 2>/dev/null; echo "0 3 * * 0 /home/ubuntu/scripts/bash/utilities/weekly_maintenance.sh" ) | crontab -

                            echo "Configuración completa en \$HOSTNAME (\$ip)"
                        EOF

                        echo ""
                        done
                    '''
                }
            }
        }

        stage('Ansible Deployment') {
            steps {
                script {
                    // 1) Volver a leer los outputs de Terraform para obtener la IP del bastión y otras variables
                    def tfOutputs = readJSON file: "tf_outputs/tf_outputs_${params.ENVIRONMENT}.json"
                    def bastionIp = tfOutputs.bastion_public_ip.value
                    
                    // 2) Copiar la clave SSH generada por Terraform a /opt/keys en la máquina de Jenkins
                    withCredentials([file(credentialsId: "AWS_SSH_KEY_${params.ENVIRONMENT}", variable: 'SSH_KEY')]) {
                        sh """
                            mkdir -p /opt/keys
                            cp ${SSH_KEY} /opt/keys/aws-key-${params.ENVIRONMENT}.pem
                            chmod 600 /opt/keys/aws-key-${params.ENVIRONMENT}.pem
                        """
                    }
                    
                    // 3) Copiar todo el directorio ../ansible desde Jenkins al bastión usando SCP
                    sh """
                        scp -o StrictHostKeyChecking=no -i /opt/keys/aws-key-${params.ENVIRONMENT}.pem \\
                            -r ${ANSIBLE_DIR} ubuntu@${bastionIp}:/home/ubuntu/ansible
                    """
                    
                    // 4) Invocar ansible-playbook desde el bastión (con ProxyJump no hace falta, porque estamos ya yendo directo al bastión.)
                    sh """
                        ssh -o StrictHostKeyChecking=no -i /opt/keys/aws-key-${params.ENVIRONMENT}.pem ubuntu@${bastionIp} \\
                            "cd ansible && ansible-playbook -i inventories/${params.ENVIRONMENT}/hosts playbooks/wordpress.yml \\
                                -e '@../tf_outputs/tf_outputs_${params.ENVIRONMENT}.json' -e 'env=${params.ENVIRONMENT}'"
                    """
                }
            }
        }
    }



    post {
        always {
            archiveArtifacts artifacts: "tf_outputs/tf_outputs_${params.ENVIRONMENT}.json"
            cleanWs()
        }
        failure {
            script {
                slackSend channel: '#infra-alerts',
                         color: 'danger',
                         tokenCredentialId: env.SLACK_TOKEN_CREDENTIAL_ID,
                         message: """Pipeline fallido: ${env.JOB_NAME} #${env.BUILD_NUMBER}
Environment: ${params.ENVIRONMENT}
Commit: ${params.GIT_SHA}
Build URL: ${env.BUILD_URL}"""
            }
        }
        success {
            script {
                slackSend channel: '#infra-alerts',
                         color: 'good',
                         tokenCredentialId: env.SLACK_TOKEN_CREDENTIAL_ID,
                         message: """Despliegue exitoso: ${env.JOB_NAME} #${env.BUILD_NUMBER}
Environment: ${params.ENVIRONMENT}
Commit: ${params.GIT_SHA}"""
            }
        }
    }
}